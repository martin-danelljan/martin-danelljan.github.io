---
layout: default
---

<head>
<style>
.image-txt-container {
  display:flex;
  align-items:center;
  flex-direction: row;
}
.item-image {
  margin: 0px 20px 0px 0px;
  width: 200px;
}
.profile-image {
  margin: 0px 0px 0px 20px;
  width: 300px;
}
</style>
</head>

<body>

<div class="image-txt-container">
	<div>
<h2>Biography</h2>
I received my Ph.D. degree from Link√∂ping University, Sweden in 2018. I am currently a postdoctoral researcher at ETH Zurich, Switzerland. My main research interests are online machine learning methods for visual tracking and video object segmentation, probabilistic models for point cloud registration, and machine learning with no or limited supervision.
<br><br><b><a href="https://www.vision.ee.ethz.ch/en/members/detail/396/">Contact details.</a></b>
	</div>
<img src="martin.jpg" class="profile-image">
</div>

	<h2>Research</h2>

Selected research projects.

<h3><b>CVPR 2019:</b> <a href="https://visionml.github.io/atom/">ATOM: Accurate Tracking by Overlap Maximization</a></h3>
<div class="image-txt-container">
<img src="atom_overview.png" class="item-image">
	<div>
In this work we primarily address the problem of performing accurate bounding box estimation for generic visual tracking. We train a target estimation module offline, conditioned on the target appearance, to predict the overlap between the object and a bounding box estimate. Furthermore, we propose a target classification component that is learned online using dedicated optimization techniques.
		<br> <b>[<a href="https://visionml.github.io/atom/">Project</a>] [<a href="https://arxiv.org/abs/1811.07628">Paper</a>] [<a href="https://github.com/visionml/pytracking">Code</a>] </b>
</div></div>

<h3><b>CVPR 2019:</b> <a href="https://arxiv.org/abs/1811.11611">A Generative Appearance Model for End-to-end Video Object Segmentation</a></h3>
<div class="image-txt-container">
<img src="AGAMEfig.png" class="item-image">
	<div>
		Here, we propose a fully end-to-end architecture for Video Object Segmentation. We introduce a generative appearance module that generates class-conditional probabilities. This provides a highly discriminative cue, which is processed in later network modules.
		<br> <b>[<a href="https://arxiv.org/abs/1811.11611">Paper</a>] [<a href="https://github.com/joakimjohnander/agame-vos">Code</a>] </b>
</div></div>

<h3><b>ECCV 2018:</b> <a href="https://arxiv.org/abs/1804.06833">Unveiling the Power of Deep Tracking</a></h3>
<div class="image-txt-container">
<img src="UPDTfig.png" class="item-image">
	<div>
In this work, we investigate the issue of utilizing deep features for tracking. We systematically study the characteristics of both deep and shallow features, and their relation to tracking accuracy and robustness. We identify the limited data and low spatial resolution as the main challenges, and propose strategies to counter these issues when integrating deep features for tracking. Furthermore, we propose a novel adaptive fusion approach that leverages the complementary properties of deep and shallow features to improve both robustness and accuracy.
		<br> <b> [<a href="https://arxiv.org/abs/1804.06833">Paper</a>] </b>
</div></div>

<h3><b>CVPR 2018:</b> <a href="https://arxiv.org/abs/1804.01495">Density Adaptive Point Set Registration</a></h3>
<div class="image-txt-container">
<img src="DAREfig.png" class="item-image">
	<div>
		We revisit the foundations of the probabilistic point cloud registration paradigm, in order to tackle the key issue of sampling density variations. Contrary to previous works, we model the underlying structure of the scene as a latent probability distribution, and thereby induce invariance to point set density changes. Both the probabilistic model of the scene and the registration parameters are inferred by minimizing the Kullback-Leibler divergence in an Expectation Maximization based framework.
		<br> <b>[<a href="https://arxiv.org/abs/1804.01495">Paper</a>] [<a href="https://github.com/felja633/DARE">Code</a>] </b>
</div></div>

<h3><b>CVPR 2017:</b> <a href="https://visionml.github.io/eco/">ECO: Efficient Convolution Operators for Tracking</a></h3>
<div class="image-txt-container">
<img src="ECOfig.png" class="item-image">
<div>In this work we tackle the key causes behind the problems of computational complexity <i>and</i> over-fitting in advanced DCF trackers. We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model; (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples; (iii) a conservative model update strategy with improved robustness and reduced complexity.
	<br><b>[<a href="https://visionml.github.io/eco/">Project</a>] [<a href="https://arxiv.org/abs/1611.09224">Paper</a>] [<a href="https://github.com/martin-danelljan/ECO">Code</a>]</b>
</div>
</div>	

	<h3><b>ECCV 2016:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html">Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="CCOTfig.png" class="item-image">
	<div>In this work we develop a theoretical framework for discriminatively learning a convolution operator in the continuous spatial domain. Our formulation enables a natural integration of multi-resolution deep feature maps. In addition, our continuous formulation is capable of accurate sub-pixel localization of the target.
		<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html">Project</a>] [<a href="https://arxiv.org/abs/1608.03773">Paper</a>] [<a href="https://github.com/martin-danelljan/Continuous-ConvOp">Code</a>]</b>
</div>
</div>
	
	<h3><b>CVPR 2016:</b> <a href="http://www.cvl.isy.liu.se/research/cogvis/colored-point-set-registration/index.html">A Probabilistic Framework for Color-Based Point Set Registration</a></h3>
<div class="image-txt-container">
<img src="ColorPCregFig.png" class="item-image">
	<div>In work we propose a probabilistic point set registration framework that exploits available <b>color</b> information associated with the points. Our method is based on a model of the joint distribution of 3D-point observations and their color information. We derive an EM algorithm for jointly estimating the model parameters and the relative transformations. The proposed model captures discriminative color information, while being computationally efficient.
		<br><b>[<a href="http://www.cvl.isy.liu.se/research/cogvis/colored-point-set-registration/index.html">Project</a>] [<a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Danelljan_A_Probabilistic_Framework_CVPR_2016_paper.pdf">Paper</a>] [<a href="https://github.com/felja633/DARE">Code</a>]</b>
</div>
</div>
	
	<h3><b>CVPR 2016:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html">Adaptive Decontamination of the Training Set: A Unified Formulation for Discriminative Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="DeconFig.png" class="item-image">
	<div>In this work we propose a unified formulation for alleviating the problem of corrupted training samples in tracking-by-detection methods. This is achieved by minimizing a joint loss over both target appearance model and the training sample quality weights. Our approach is generic and can be integrated into any discriminative tracking framework.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html">Project and Code</a>] [<a href="https://arxiv.org/abs/1609.06118">Paper</a>]</b>
</div>
</div>
	
	<h3><b>ICCV 2015:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/index.html">Learning Spatially Regularized Correlation Filters for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="SRDCFfig_small.png" class="item-image">
	<div>In this work we propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. This effectively mitigates the unwanted boundary effects, which limits the performance of standard correlation based trackers. The SRDCF tracker <b>won</b> the recent OpenCV Challenge and achieved the <b>best result</b> in the VOT-TIR2015 challenge.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/index.html">Project and Code</a>] [<a href="https://arxiv.org/abs/1608.05571">Paper</a>]</b>
</div>
</div>
	
	<h3><b>TPAMI, BMVC 2014:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/scalvistrack/index.html">Scale Estimation for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="DSSTpyramidFig_small.png" class="item-image">
	<div>Here we investigate the problem of accurate and fast scale estimation for visual tracking. The proposed Discriminative Scale Space Tracker (DSST) <b>won</b> the Visual Object Tracking (VOT) 2014 challenge.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/scalvistrack/index.html">Project and Code</a>] [<a href="https://arxiv.org/abs/1609.06141">Paper, PAMI</a>] [<a href="http://www.bmva.org/bmvc/2014/files/paper038.pdf">Paper, BMVC</a>]</b>
</div>
</div>
	
	<h3><b>CVPR 2014:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html">Adaptive Color Attributes for Real-Time Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="CNfig_small.png" class="item-image">
	<div>In this work, we investigated how to incorporate color information into visual tracking.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html">Project and Code</a>] [<a href="http://openaccess.thecvf.com/content_cvpr_2014/papers/Danelljan_Adaptive_Color_Attributes_2014_CVPR_paper.pdf">Paper</a>]</b>
</div>
</div>

<h2>Publications</h2>
See my <a href="https://scholar.google.com/citations?user=NCSSpMkAAAAJ&hl=en">Google Scholar profile</a> or my <a href="http://dblp.uni-trier.de/pers/hd/d/Danelljan:Martin">dblp page</a> for a list of publications.

</body>

