---
layout: default
---

<head>
<style>
.image-txt-container {
  display:flex;
  align-items:center;
  flex-direction: row;
}
.item-image {
  margin: 0px 20px 0px 0px;
  width: 200px;
}
.profile-image {
  margin: 0px 0px 0px 20px;
  width: 300px;
}
</style>
</head>

<body>

<div class="image-txt-container">
	<div>
<h2>Biography</h2>
I received my Ph.D. degree from Link√∂ping University, Sweden in 2018. I am currently a postdoctoral researcher at ETH Zurich, Switzerland. My main research interests are developing machine learning methods for meta and online learning applications, such as visual tracking and video object segmentation. I am also interested in generative models and machine learning with no or limited supervision.
<br><br>
<b>News:</b> Four papers accepted in CVPR 2020.
<br><br><b>[<a href="https://www.vision.ee.ethz.ch/en/members/detail/396/">Contact details</a>] [<a href="https://scholar.google.com/citations?user=NCSSpMkAAAAJ&hl=en">Google Scholar</a>] [<a href="http://dblp.uni-trier.de/pers/hd/d/Danelljan:Martin">dblp</a>] </b>
	</div>
<img src="martin.jpg" class="profile-image">
</div>

	<h2>Research</h2>

Selected research projects and papers.


<h3><b>CVPR 2020:</b> <a href="https://github.com/PruneTruong/GLU-Net">GLU-Net: Global-Local Universal Network for Dense Flow and Correspondences</a></h3>
<div class="image-txt-container">
<img src="glunetfig.jpg" class="item-image">
	<div>
In this work, we propose a universal network architecture for establishing dense correspondences between two images. The same network achieves state-of-the-art performance for geometric and semantic matching as well as optical flow. We achieve both high accuracy and robustness to large displacements by investigating the combined use of global and local correlation layers. We further propose an adaptive resolution strategy, allowing our network to operate on virtually any input image resolution.
		<br> <b>[<a href="https://github.com/PruneTruong/GLU-Net">Project</a>] [<a href="https://arxiv.org/abs/1912.05524">Paper</a>] [<a href="https://github.com/PruneTruong/GLU-Net">Code</a>] </b>
</div></div>


<h3><b>CVPR 2020:</b> <a href="https://github.com/visionml/pytracking">Probabilistic Regression for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="prdimpfig.png" class="item-image">
	<div>
This work proposes a general formulation for probabilistic regression, which is then applied to visual tracking. Our network predicts the conditional probability density of the target state given an input image. The probability density is flexibly parametrized by the neural network itself. Crucially, our formulation is capable of modeling label noise stemming from inaccurate annotations and ambiguities in the task. The regression network is trained by directly minimizing the Kullback-Leibler divergence. 
		<br> <b>[<a href="https://github.com/visionml/pytracking">Project</a>] [<a href="https://arxiv.org/abs/2003.12565">Paper</a>] [<a href="https://github.com/visionml/pytracking">Code</a>] </b>
</div></div>


<h3><b>CVPR 2020:</b> <a href="https://github.com/andr345/frtm-vos">Learning Fast and Robust Target Models for Video Object Segmentation</a></h3>
<div class="image-txt-container">
<img src="frtmfig.jpg" class="item-image">
	<div>
This work proposes a novel architecture for Video Object Segmentation (VOS). It integrates a target appearance model, which consists of a light-weight module that is learned during the inference stage. To this end, we employ fast optimization techniques in order to learn to predict a coarse but robust target segmentation. The coarse mask is processed by a segmentation network, that is exclusively trained offline to predict a high quality segmentation. Our method is fast, easily trainable and remains highly effective in cases of limited training data. 
		<br> <b>[<a href="https://github.com/andr345/frtm-vos">Project</a>] [<a href="https://arxiv.org/abs/2003.00908">Paper</a>] [<a href="https://github.com/andr345/frtm-vos">Code</a>] </b>
</div></div>


<h3><b>CVPR 2020:</b> <a href="https://github.com/vaesl/IP-Net">Learning Human-Object Interaction Detection using Interaction Points</a></h3>
<div class="image-txt-container">
<img src="hoifig.jpg" class="item-image">
	<div>
Human-object interaction (HOI) detection strives to localize both the human and an object as well as the identification of complex interactions between them.  Most existing HOI detection approaches are instance-centric where interactions between all possible human-object pairs are predicted based on appearance features and coarse spatial information. We argue that appearance features alone are insufficient to capture complex human-object interactions. In this paper, we therefore propose a novel fully-convolutional approach that directly detects the interactions between human-object pairs. Our network predicts interaction points, which directly localize and classify the interaction. Paired with the densely predicted interaction vectors, the interactions are associated with human and object detections to obtain final predictions.
		<br> <b>[<a href="https://github.com/vaesl/IP-Net">Project</a>] [<a href="https://arxiv.org/abs/2003.14023">Paper</a>] [<a href="https://github.com/vaesl/IP-Net">Code</a>] </b>
</div></div>


<h3><b>ICCV 2019:</b> <a href="https://visionml.github.io/dimp/">Learning Discriminative Model Prediction for Tracking</a></h3>
<div class="image-txt-container">
<img src="dimpfig.png" class="item-image">
	<div>
In this work, we develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, while running at over 40 FPS.
		<br> <b>[<a href="https://visionml.github.io/dimp/">Project</a>] [<a href="https://arxiv.org/abs/1904.07220">Paper</a>] [<a href="https://github.com/visionml/pytracking">Code</a>] </b>
</div></div>

<h3><b>ICCV 2019:</b> <a href="https://arxiv.org/abs/1908.00855">Learning the Model Update for Siamese Trackers.</a></h3>
<div class="image-txt-container">
<img src="updatenet.png" class="item-image">
	<div>
We propose to replace the handcrafted update function in Siamese trackers with a learnable update mechanism. We use a convolutional neural network, called UpdateNet, which given the initial template, the accumulated template and the template of the current frame estimates the optimal template for the next frame.
		<br> <b>[<a href="https://arxiv.org/abs/1908.00855">Paper</a>] [<a href="https://github.com/zhanglichao/updatenet">Code</a>] </b>
</div></div>

<h3><b>CVPR 2019:</b> <a href="https://visionml.github.io/atom/">ATOM: Accurate Tracking by Overlap Maximization</a></h3>
<div class="image-txt-container">
<img src="atom_overview.png" class="item-image">
	<div>
In this work we primarily address the problem of performing accurate bounding box estimation for generic visual tracking. We train a target estimation module offline, conditioned on the target appearance, to predict the overlap between the object and a bounding box estimate. Furthermore, we propose a target classification component that is learned online using dedicated optimization techniques.
		<br> <b>[<a href="https://visionml.github.io/atom/">Project</a>] [<a href="https://arxiv.org/abs/1811.07628">Paper</a>] [<a href="https://github.com/visionml/pytracking">Code</a>] </b>
</div></div>

<h3><b>CVPR 2019:</b> <a href="https://arxiv.org/abs/1811.11611">A Generative Appearance Model for End-to-end Video Object Segmentation</a></h3>
<div class="image-txt-container">
<img src="AGAMEfig.png" class="item-image">
	<div>
		Here, we propose a fully end-to-end architecture for Video Object Segmentation. We introduce a generative appearance module that generates class-conditional probabilities. This provides a highly discriminative cue, which is processed in later network modules.
		<br> <b>[<a href="https://arxiv.org/abs/1811.11611">Paper</a>] [<a href="https://github.com/joakimjohnander/agame-vos">Code</a>] </b>
</div></div>

<h3><b>ECCV 2018:</b> <a href="https://arxiv.org/abs/1804.06833">Unveiling the Power of Deep Tracking</a></h3>
<div class="image-txt-container">
<img src="UPDTfig.png" class="item-image">
	<div>
In this work, we investigate the issue of utilizing deep features for tracking. We systematically study the characteristics of both deep and shallow features, and their relation to tracking accuracy and robustness. We identify the limited data and low spatial resolution as the main challenges, and propose strategies to counter these issues when integrating deep features for tracking. Furthermore, we propose a novel adaptive fusion approach that leverages the complementary properties of deep and shallow features to improve both robustness and accuracy.
		<br> <b> [<a href="https://arxiv.org/abs/1804.06833">Paper</a>] </b>
</div></div>

<h3><b>CVPR 2018:</b> <a href="https://arxiv.org/abs/1804.01495">Density Adaptive Point Set Registration</a></h3>
<div class="image-txt-container">
<img src="DAREfig.png" class="item-image">
	<div>
		We revisit the foundations of the probabilistic point cloud registration paradigm, in order to tackle the key issue of sampling density variations. Contrary to previous works, we model the underlying structure of the scene as a latent probability distribution, and thereby induce invariance to point set density changes. Both the probabilistic model of the scene and the registration parameters are inferred by minimizing the Kullback-Leibler divergence in an Expectation Maximization based framework.
		<br> <b>[<a href="https://arxiv.org/abs/1804.01495">Paper</a>] [<a href="https://github.com/felja633/DARE">Code</a>] </b>
</div></div>

<h3><b>CVPR 2017:</b> <a href="https://visionml.github.io/eco/">ECO: Efficient Convolution Operators for Tracking</a></h3>
<div class="image-txt-container">
<img src="ECOfig.png" class="item-image">
<div>In this work we tackle the key causes behind the problems of computational complexity <i>and</i> over-fitting in advanced DCF trackers. We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model; (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples; (iii) a conservative model update strategy with improved robustness and reduced complexity.
	<br><b>[<a href="https://visionml.github.io/eco/">Project</a>] [<a href="https://arxiv.org/abs/1611.09224">Paper</a>] [<a href="https://github.com/martin-danelljan/ECO">Code</a>]</b>
</div>
</div>	

	<h3><b>ECCV 2016:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html">Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="CCOTfig.png" class="item-image">
	<div>In this work we develop a theoretical framework for discriminatively learning a convolution operator in the continuous spatial domain. Our formulation enables a natural integration of multi-resolution deep feature maps. In addition, our continuous formulation is capable of accurate sub-pixel localization of the target.
		<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html">Project</a>] [<a href="https://arxiv.org/abs/1608.03773">Paper</a>] [<a href="https://github.com/martin-danelljan/Continuous-ConvOp">Code</a>]</b>
</div>
</div>
	
	<h3><b>CVPR 2016:</b> <a href="http://www.cvl.isy.liu.se/research/cogvis/colored-point-set-registration/index.html">A Probabilistic Framework for Color-Based Point Set Registration</a></h3>
<div class="image-txt-container">
<img src="ColorPCregFig.png" class="item-image">
	<div>In work we propose a probabilistic point set registration framework that exploits available <b>color</b> information associated with the points. Our method is based on a model of the joint distribution of 3D-point observations and their color information. We derive an EM algorithm for jointly estimating the model parameters and the relative transformations. The proposed model captures discriminative color information, while being computationally efficient.
		<br><b>[<a href="http://www.cvl.isy.liu.se/research/cogvis/colored-point-set-registration/index.html">Project</a>] [<a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Danelljan_A_Probabilistic_Framework_CVPR_2016_paper.pdf">Paper</a>] [<a href="https://github.com/felja633/DARE">Code</a>]</b>
</div>
</div>
	
	<h3><b>CVPR 2016:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html">Adaptive Decontamination of the Training Set: A Unified Formulation for Discriminative Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="DeconFig.png" class="item-image">
	<div>In this work we propose a unified formulation for alleviating the problem of corrupted training samples in tracking-by-detection methods. This is achieved by minimizing a joint loss over both target appearance model and the training sample quality weights. Our approach is generic and can be integrated into any discriminative tracking framework.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html">Project and Code</a>] [<a href="https://arxiv.org/abs/1609.06118">Paper</a>]</b>
</div>
</div>
	
	<h3><b>ICCV 2015:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/index.html">Learning Spatially Regularized Correlation Filters for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="SRDCFfig_small.png" class="item-image">
	<div>In this work we propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. This effectively mitigates the unwanted boundary effects, which limits the performance of standard correlation based trackers. The SRDCF tracker <b>won</b> the recent OpenCV Challenge and achieved the <b>best result</b> in the VOT-TIR2015 challenge.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/index.html">Project and Code</a>] [<a href="https://arxiv.org/abs/1608.05571">Paper</a>]</b>
</div>
</div>
	
	<h3><b>TPAMI, BMVC 2014:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/scalvistrack/index.html">Scale Estimation for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="DSSTpyramidFig_small.png" class="item-image">
	<div>Here we investigate the problem of accurate and fast scale estimation for visual tracking. The proposed Discriminative Scale Space Tracker (DSST) <b>won</b> the Visual Object Tracking (VOT) 2014 challenge.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/scalvistrack/index.html">Project and Code</a>] [<a href="https://arxiv.org/abs/1609.06141">Paper, PAMI</a>] [<a href="http://www.bmva.org/bmvc/2014/files/paper038.pdf">Paper, BMVC</a>]</b>
</div>
</div>
	
	<h3><b>CVPR 2014:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html">Adaptive Color Attributes for Real-Time Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="CNfig_small.png" class="item-image">
	<div>In this work, we investigated how to incorporate color information into visual tracking.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html">Project and Code</a>] [<a href="http://openaccess.thecvf.com/content_cvpr_2014/papers/Danelljan_Adaptive_Color_Attributes_2014_CVPR_paper.pdf">Paper</a>]</b>
</div>
</div>

<h2>Publications</h2>
See my <a href="https://scholar.google.com/citations?user=NCSSpMkAAAAJ&hl=en">Google Scholar profile</a> or my <a href="http://dblp.uni-trier.de/pers/hd/d/Danelljan:Martin">dblp page</a> for a list of publications.

</body>

